{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquire (acquire.py)\n",
    "\n",
    "## Zillow\n",
    "\n",
    "For the following, iterate through the steps you would take to create functions: Write the code to do the following in a jupyter notebook, test it, convert to functions, then create the file to house those functions.\n",
    "\n",
    "You will have a zillow.ipynb file and a helper file for each section in the pipeline.\n",
    "\n",
    "### acquire & summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Acquire data from mySQL using the python module to connect and query. You will want to end with a single dataframe. Make sure to include: the logerror, all fields related to the properties that are available. You will end up using all the tables in the database.\n",
    "    - ***Be sure to do the correct join (inner, outer, etc.). We do not want to eliminate properties purely because they may have a null value for `airconditioningtypeid`.***\n",
    "    - Only include properties with a transaction in 2017, and include only the last transaction for each properity (so no duplicate property ID's), along with zestimate error and date of transaction.\n",
    "    - Only include properties that include a latitude and longitude value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from env import host, user, password\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### Acquire Zillow Data ######################\n",
    "\n",
    "def get_connection(db, user=user, host=host, password=password):\n",
    "    '''\n",
    "    This function uses my info from my env file to\n",
    "    create a connection url to access the Codeup db.\n",
    "    '''\n",
    "    return f'mysql+pymysql://{user}:{password}@{host}/{db}'\n",
    "    \n",
    "def new_zillow_data():\n",
    "    '''\n",
    "    This function reads the zillow data from the Codeup db into a df,\n",
    "    write it to a csv file, and returns the df.\n",
    "    '''\n",
    "    sql_query = '''\n",
    "            SELECT *\n",
    "            FROM properties_2017 as prop \n",
    "            INNER JOIN (\n",
    "                    SELECT id, p.parcelid, logerror, transactiondate\n",
    "                    FROM predictions_2017 AS p\n",
    "                    INNER JOIN (\n",
    "                            SELECT parcelid,  MAX(transactiondate) AS max_date\n",
    "                            FROM predictions_2017 \n",
    "                            GROUP BY (parcelid)\n",
    "                    ) AS sub\n",
    "                        ON p.parcelid = sub.parcelid\n",
    "                    WHERE p.transactiondate = sub.max_date\n",
    "            ) AS subq\n",
    "                ON prop.id = subq.id;\n",
    "                 '''\n",
    "    df = pd.read_sql(sql_query, get_connection('zillow'))\n",
    "    df.to_csv('zillow.csv')\n",
    "    return df\n",
    "\n",
    "def get_zillow_data(cached=False):\n",
    "    '''\n",
    "    This function reads in titanic data from Codeup database if cached == False \n",
    "    or if cached == True reads in zillow df from a csv file, returns df\n",
    "    '''\n",
    "    if cached or os.path.isfile('zillow_df.csv') == False:\n",
    "        df = new_zillow_data()\n",
    "    else:\n",
    "        df = pd.read_csv('zillow_df.csv', index_col=0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Summarize your data (summary stats, info, dtypes, shape, distributions, value_counts, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_zillow_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Write a function that takes in a dataframe of observations and attributes and returns a dataframe where each row is an attribute name, the first column is the number of rows with missing values for that attribute, and the second column is percent of total rows that have missing values for that attribute. Run the function and document takeaways from this on how you want to handle missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_rows_df(df):\n",
    "    '''Takes in a dataframe of observations and attributes \n",
    "    and returns a dataframe where each row is an attribute name, \n",
    "    the first column is the number of rows with missing values \n",
    "    for that attribute, and the second column is percent of total \n",
    "    rows that have missing values for that attribute'''\n",
    "    d = {'num_rows_missing': df.isna().sum(), \n",
    "         'pct_rows_missing': df.isna().sum()/len(df)}\n",
    "    new_df = pd.DataFrame(data=d)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mr = missing_rows_df(df)\n",
    "df_mr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_no_missing = df_mr[df_mr.num_rows_missing == 0].T.columns.to_list()\n",
    "cols_no_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_more_than_50pct_missing = df_mr[df_mr.pct_rows_missing > .5].T.columns.to_list()\n",
    "cols_more_than_50pct_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "takeaways: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Write a function that takes in a dataframe and returns a dataframe with 3 columns: the number of columns missing, percent of columns missing, and number of rows with n columns missing. Run the function and document takeaways from this on how you want to handle missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_cols_df(df):\n",
    "    '''takes in a dataframe and returns a dataframe \n",
    "    with 3 columns: the number of columns missing, \n",
    "    percent of columns missing, and number of rows \n",
    "    with n columns missing'''\n",
    "    d = {'num_cols_missing': df.isna().sum(axis=1), \n",
    "         'pct_cols_missing': df.isna().sum(axis=1)/len(df.columns),\n",
    "         'num_rows': 'fill'}\n",
    "    new_df = pd.DataFrame(data=d)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_cols_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum(axis=1).value_counts().index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare\n",
    "\n",
    "1. Remove any properties that are likely to be something other than single unit properties. (e.g. no duplexes, no land/lot, ...). There are multiple ways to estimate that a property is a single unit, and there is not a single \"right\" answer. But for this exercise, do not purely filter by unitcnt as we did previously. Add some new logic that will reduce the number of properties that are falsely removed. You might want to use # bedrooms, square feet, unit type or the like to then identify those with unitcnt not defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop = pd.Series([260, 261, 263, 273, 275, 279, 276])\n",
    "df_su = df[(df.propertylandusetypeid.isin(prop))]\n",
    "df_su.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a function that will drop rows or columns based on the percent of values that are missing: handle_missing_values(df, prop_required_column, prop_required_row).\n",
    "\n",
    "    - The input:\n",
    "        - A dataframe\n",
    "        - A number between 0 and 1 that represents the proportion, for each column, of rows with non-missing values required to keep the column. i.e. if prop_required_column = .6, then you are requiring a column to have at least 60% of values not-NA (no more than 40% missing).\n",
    "        - A number between 0 and 1 that represents the proportion, for each row, of columns/variables with non-missing values required to keep the row. For example, if prop_required_row = .75, then you are requiring a row to have at least 75% of variables with a non-missing value (no more that 25% missing).\n",
    "    - The output:\n",
    "        - The dataframe with the columns and rows dropped as indicated. Be sure to drop the columns prior to the rows in your function.\n",
    "    - hint:\n",
    "        - Look up the dropna documentation.\n",
    "        - You will want to compute a threshold from your input values (prop_required) and total number of rows or columns.\n",
    "        - Make use of inplace, i.e. inplace=True/False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(df, prop_required_column, prop_required_row):\n",
    "    threshold = int(round(prop_required_column*len(df.index),0))\n",
    "    df.dropna(axis=1, thresh=threshold, inplace=True)\n",
    "    threshold = int(round(prop_required_row*len(df.columns),0))\n",
    "    df.dropna(axis=0, thresh=threshold, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = handle_missing_values(df_su, .85, .75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Decide how to handle the remaining missing values:\n",
    "\n",
    "    - Fill with constant value.\n",
    "    - Impute with mean, median, mode.\n",
    "    - Drop row/column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.dropna()\n",
    "df2.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mall Customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Acquire data from mall_customers.customers in mysql database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### Acquire Zillow Data ######################\n",
    "\n",
    "def get_connection(db, user=user, host=host, password=password):\n",
    "    '''\n",
    "    This function uses my info from my env file to\n",
    "    create a connection url to access the Codeup db.\n",
    "    '''\n",
    "    return f'mysql+pymysql://{user}:{password}@{host}/{db}'\n",
    "    \n",
    "def new_mall_data():\n",
    "    '''\n",
    "    This function reads the mall data from the Codeup db into a df,\n",
    "    write it to a csv file, and returns the df.\n",
    "    '''\n",
    "    sql_query = '''SELECT * FROM customers\n",
    "                 '''\n",
    "    df = pd.read_sql(sql_query, get_connection('mall_customers'))\n",
    "    df.to_csv('mall_customers.csv')\n",
    "    return df\n",
    "\n",
    "def get_mall_data(cached=False):\n",
    "    '''\n",
    "    This function reads in mall data from Codeup database if cached == False \n",
    "    or if cached == True reads in mall df from a csv file, returns df\n",
    "    '''\n",
    "    if cached or os.path.isfile('mall_df.csv') == False:\n",
    "        df = new_mall_data()\n",
    "    else:\n",
    "        df = pd.read_csv('mall_df.csv', index_col=0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mall = get_mall_data()\n",
    "mall.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Summarize data (include distributions and descriptive statistics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mall.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mall.drop(columns='customer_id').describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mall.drop(columns='customer_id').hist()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Detect outliers using IQR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_upper_outliers(s, k):\n",
    "    '''\n",
    "    Given a series and a cutoff value, k, returns the upper outliers for the\n",
    "    series.\n",
    "\n",
    "    The values returned will be either 0 (if the point is not an outlier), or a\n",
    "    number that indicates how far away from the upper bound the observation is.\n",
    "    '''\n",
    "    q1, q3 = s.quantile([.25, .75])\n",
    "    iqr = q3 - q1\n",
    "    upper_bound = q3 + k * iqr\n",
    "    return s.apply(lambda x: max([x - upper_bound, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_upper_outliers(mall.age, 1.5).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_upper_outliers(mall.spending_score, 1.5).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_upper_outliers(mall.annual_income, 1.5).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_upper_outliers(mall.annual_income, 1.5).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(get_upper_outliers(mall.annual_income, 1.5) == 4.25).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(get_upper_outliers(mall.annual_income, 1.5) == 4.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lower_outliers(s, k):\n",
    "    '''\n",
    "    Given a series and a cutoff value, k, returns the lower outliers for the\n",
    "    series.\n",
    "\n",
    "    The values returned will be either 0 (if the point is not an outlier), or a\n",
    "    number that indicates how far away from the lower bound the observation is.\n",
    "    '''\n",
    "    q1, q3 = s.quantile([.25, .75])\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - k * iqr\n",
    "    return s.apply(lambda x: max([lower_bound - x, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_lower_outliers(mall.age, 1.5).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_lower_outliers(mall.annual_income, 1.5).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_lower_outliers(mall.spending_score, 1.5).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Split data (train, validate, and test split)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mall_split(df):\n",
    "    train_validate, test = train_test_split(df, test_size=.2, \n",
    "                                        random_state=123)\n",
    "    train, validate = train_test_split(train_validate, test_size=.3, \n",
    "                                   random_state=123)\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate, test = mall_split(mall)\n",
    "train.shape, validate.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Encode categorical columns using a one hot encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(train, validate, test, cols):\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    ohe = OneHotEncoder(sparse=False, categories='auto')\n",
    "\n",
    "    # fit and transform train to create an array\n",
    "    train_matrix = ohe.fit_transform(train[cols])\n",
    "\n",
    "    # transform validate and test to create arrays\n",
    "    validate_matrix = ohe.transform(validate[cols])\n",
    "    test_matrix = ohe.transform(test[cols])\n",
    "    \n",
    "    # convert arrays to dataframes of encoded column\n",
    "    train_ohe = pd.DataFrame(train_matrix, columns=ohe.categories_[0], index=train.index)\n",
    "    validate_ohe = pd.DataFrame(validate_matrix, columns=ohe.categories_[0], index=validate.index)\n",
    "    test_ohe = pd.DataFrame(test_matrix, columns=ohe.categories_[0], index=test.index)\n",
    "\n",
    "    # join to the original datasets\n",
    "    train = train.join(train_ohe)\n",
    "    validate = validate.join(validate_ohe)\n",
    "    test = test.join(test_ohe)\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate, test = one_hot_encoder(train, validate, test, ['gender'])\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Handles missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_scale = ['age', 'annual_income', 'spending_score', 'Male']\n",
    "\n",
    "X_train = train[cols_to_scale]\n",
    "X_validate = validate[cols_to_scale]\n",
    "X_test = test[cols_to_scale]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scale(X_train, X_validate, X_test):\n",
    "    # import scaler\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    # Create scaler object\n",
    "    scaler = MinMaxScaler(copy=True).fit(X_train)\n",
    "    \n",
    "    # tranform into scaled data (arrays)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_validate_scaled = scaler.transform(X_validate)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Create dataframes out of the scaled arrays that were generated by the scaler tranform.\n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled, \n",
    "                              columns=X_train.columns.values).\\\n",
    "                            set_index([X_train.index.values])\n",
    "\n",
    "    X_validate_scaled = pd.DataFrame(X_validate_scaled, \n",
    "                                columns=X_validate.columns.values).\\\n",
    "                            set_index([X_validate.index.values])\n",
    "\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled, \n",
    "                                columns=X_test.columns.values).\\\n",
    "                            set_index([X_test.index.values])\n",
    "    return X_train_scaled, X_validate_scaled, X_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled, X_validate_scaled, X_test_scaled = min_max_scale(X_train, X_validate, X_test)\n",
    "X_train_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Final .py files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>parcelid</th>\n",
       "      <th>bathroomcnt</th>\n",
       "      <th>bedroomcnt</th>\n",
       "      <th>calculatedbathnbr</th>\n",
       "      <th>calculatedfinishedsquarefeet</th>\n",
       "      <th>finishedsquarefeet12</th>\n",
       "      <th>fips</th>\n",
       "      <th>fullbathcnt</th>\n",
       "      <th>latitude</th>\n",
       "      <th>...</th>\n",
       "      <th>structuretaxvaluedollarcnt</th>\n",
       "      <th>taxvaluedollarcnt</th>\n",
       "      <th>assessmentyear</th>\n",
       "      <th>landtaxvaluedollarcnt</th>\n",
       "      <th>taxamount</th>\n",
       "      <th>censustractandblock</th>\n",
       "      <th>id</th>\n",
       "      <th>parcelid</th>\n",
       "      <th>logerror</th>\n",
       "      <th>transactiondate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>337</td>\n",
       "      <td>17051828</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4969.0</td>\n",
       "      <td>4969.0</td>\n",
       "      <td>6111.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>34433200.0</td>\n",
       "      <td>...</td>\n",
       "      <td>779712.0</td>\n",
       "      <td>2228473.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1448761.0</td>\n",
       "      <td>24556.86</td>\n",
       "      <td>6.111001e+13</td>\n",
       "      <td>337</td>\n",
       "      <td>14688177</td>\n",
       "      <td>0.006783</td>\n",
       "      <td>2017-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>338</td>\n",
       "      <td>17052152</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>6111.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>34464778.0</td>\n",
       "      <td>...</td>\n",
       "      <td>319803.0</td>\n",
       "      <td>794940.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>475137.0</td>\n",
       "      <td>8571.20</td>\n",
       "      <td>6.111001e+13</td>\n",
       "      <td>338</td>\n",
       "      <td>10930942</td>\n",
       "      <td>0.054386</td>\n",
       "      <td>2017-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>339</td>\n",
       "      <td>17052992</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>780.0</td>\n",
       "      <td>780.0</td>\n",
       "      <td>6111.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>34450686.0</td>\n",
       "      <td>...</td>\n",
       "      <td>70231.0</td>\n",
       "      <td>88731.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>18500.0</td>\n",
       "      <td>1652.36</td>\n",
       "      <td>6.111001e+13</td>\n",
       "      <td>339</td>\n",
       "      <td>17246711</td>\n",
       "      <td>0.011061</td>\n",
       "      <td>2017-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>340</td>\n",
       "      <td>17053038</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1250.0</td>\n",
       "      <td>1250.0</td>\n",
       "      <td>6111.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>34448639.0</td>\n",
       "      <td>...</td>\n",
       "      <td>79575.0</td>\n",
       "      <td>233424.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>153849.0</td>\n",
       "      <td>3191.16</td>\n",
       "      <td>6.111001e+13</td>\n",
       "      <td>340</td>\n",
       "      <td>12559816</td>\n",
       "      <td>0.061977</td>\n",
       "      <td>2017-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>341</td>\n",
       "      <td>17053061</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1404.0</td>\n",
       "      <td>1404.0</td>\n",
       "      <td>6111.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>34450089.0</td>\n",
       "      <td>...</td>\n",
       "      <td>99853.0</td>\n",
       "      <td>313844.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>213991.0</td>\n",
       "      <td>3987.00</td>\n",
       "      <td>6.111001e+13</td>\n",
       "      <td>341</td>\n",
       "      <td>14673239</td>\n",
       "      <td>0.010089</td>\n",
       "      <td>2017-01-03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  parcelid  bathroomcnt  bedroomcnt  calculatedbathnbr  \\\n",
       "336  337  17051828          4.0         4.0                4.0   \n",
       "337  338  17052152          2.0         3.0                2.0   \n",
       "338  339  17052992          1.0         2.0                1.0   \n",
       "339  340  17053038          2.0         2.0                2.0   \n",
       "340  341  17053061          1.0         2.0                1.0   \n",
       "\n",
       "     calculatedfinishedsquarefeet  finishedsquarefeet12    fips  fullbathcnt  \\\n",
       "336                        4969.0                4969.0  6111.0          4.0   \n",
       "337                        1800.0                1800.0  6111.0          2.0   \n",
       "338                         780.0                 780.0  6111.0          1.0   \n",
       "339                        1250.0                1250.0  6111.0          2.0   \n",
       "340                        1404.0                1404.0  6111.0          1.0   \n",
       "\n",
       "       latitude  ...  structuretaxvaluedollarcnt  taxvaluedollarcnt  \\\n",
       "336  34433200.0  ...                    779712.0          2228473.0   \n",
       "337  34464778.0  ...                    319803.0           794940.0   \n",
       "338  34450686.0  ...                     70231.0            88731.0   \n",
       "339  34448639.0  ...                     79575.0           233424.0   \n",
       "340  34450089.0  ...                     99853.0           313844.0   \n",
       "\n",
       "    assessmentyear  landtaxvaluedollarcnt  taxamount  censustractandblock  \\\n",
       "336         2016.0              1448761.0   24556.86         6.111001e+13   \n",
       "337         2016.0               475137.0    8571.20         6.111001e+13   \n",
       "338         2016.0                18500.0    1652.36         6.111001e+13   \n",
       "339         2016.0               153849.0    3191.16         6.111001e+13   \n",
       "340         2016.0               213991.0    3987.00         6.111001e+13   \n",
       "\n",
       "      id  parcelid  logerror  transactiondate  \n",
       "336  337  14688177  0.006783       2017-01-03  \n",
       "337  338  10930942  0.054386       2017-01-03  \n",
       "338  339  17246711  0.011061       2017-01-03  \n",
       "339  340  12559816  0.061977       2017-01-03  \n",
       "340  341  14673239  0.010089       2017-01-03  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from wrangle_zillow import wrangle_zillow\n",
    "\n",
    "wrangle_zillow().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>annual_income</th>\n",
       "      <th>spending_score</th>\n",
       "      <th>Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.403846</td>\n",
       "      <td>0.442623</td>\n",
       "      <td>0.957447</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.519231</td>\n",
       "      <td>0.319672</td>\n",
       "      <td>0.553191</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.590164</td>\n",
       "      <td>0.787234</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.942308</td>\n",
       "      <td>0.032787</td>\n",
       "      <td>0.138298</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.057692</td>\n",
       "      <td>0.147541</td>\n",
       "      <td>0.851064</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          age  annual_income  spending_score  Male\n",
       "123  0.403846       0.442623        0.957447   1.0\n",
       "76   0.519231       0.319672        0.553191   0.0\n",
       "171  0.192308       0.590164        0.787234   1.0\n",
       "10   0.942308       0.032787        0.138298   1.0\n",
       "35   0.057692       0.147541        0.851064   0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from wrangle_mall import wrangle_mall\n",
    "\n",
    "train, validate, test = wrangle_mall()\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
